{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azerbaijani ASR Model Training - Production Pipeline\n",
    "\n",
    "**End-to-end ASR training workflow following industry best practices**\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete pipeline for training an Automatic Speech Recognition model for Azerbaijani language using Whisper architecture.\n",
    "\n",
    "## Project Structure\n",
    "```\n",
    ".\n",
    "├── charts/      # All visualizations (PNG/SVG)\n",
    "├── outputs/     # Metrics, tables, evaluation summaries\n",
    "├── artifacts/   # Trained models, processors, configs\n",
    "└── data/        # Dataset cache\n",
    "```\n",
    "\n",
    "## Features\n",
    "- Reproducible training with fixed random seeds\n",
    "- Proper train/val/test splits with no data leakage\n",
    "- Comprehensive evaluation metrics and visualizations\n",
    "- Model versioning and artifact management\n",
    "- Production-ready code with error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Experiment: whisper_azerbaijani_20260111_153802\n",
      "======================================================================\n",
      "Mode: SAMPLE\n",
      "Model: openai/whisper-tiny\n",
      "Dataset: LocalDoc/azerbaijani_asr\n",
      "Random Seed: 42\n",
      "\n",
      "Configuration saved to: outputs/whisper_azerbaijani_20260111_153802_config.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Central configuration for entire pipeline\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Project directories\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "CHARTS_DIR = PROJECT_ROOT / \"charts\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# Create directories\n",
    "for directory in [CHARTS_DIR, OUTPUTS_DIR, ARTIFACTS_DIR, DATA_DIR]:\n",
    "    directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"openai/whisper-tiny\",  # CHANGED: Using tiny for faster download\n",
    "    \"language\": \"azerbaijani\",\n",
    "    \"task\": \"transcribe\",\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"dataset_name\": \"LocalDoc/azerbaijani_asr\",\n",
    "    \"sample_mode\": True,  # Set False for full training\n",
    "    \"sample_size\": 500,   # Number of samples in sample mode\n",
    "    \"sampling_rate\": 16000,\n",
    "    \n",
    "    # Data splits\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"max_steps\": -1,  # -1 for full epochs\n",
    "    \"fp16\": True,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 500,\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_total_limit\": 3,\n",
    "    \n",
    "    # Reproducibility\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"experiment_name\": f\"whisper_azerbaijani_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "}\n",
    "\n",
    "# Adjust config for sample mode\n",
    "if CONFIG[\"sample_mode\"]:\n",
    "    CONFIG.update({\n",
    "        \"batch_size\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"max_steps\": 100,\n",
    "        \"eval_steps\": 50,\n",
    "        \"save_steps\": 50,\n",
    "        \"warmup_steps\": 20,\n",
    "    })\n",
    "\n",
    "# Save configuration\n",
    "config_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2, default=str)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Experiment: {CONFIG['experiment_name']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mode: {'SAMPLE' if CONFIG['sample_mode'] else 'FULL TRAINING'}\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Dataset: {CONFIG['dataset_name']}\")\n",
    "print(f\"Random Seed: {CONFIG['random_seed']}\")\n",
    "print(f\"\\nConfiguration saved to: {config_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SSL verification disabled (corporate network compatibility)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SSL Configuration for Corporate Networks\n",
    "# ============================================================\n",
    "\n",
    "import ssl\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Disable Xet storage and SSL verification\n",
    "os.environ['HF_HUB_DISABLE_XET'] = '1'\n",
    "os.environ['HF_HUB_DISABLE_SSL_VERIFY'] = '1'\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "sys.modules['hf_xet'] = None\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "warnings.filterwarnings('ignore', message='Unverified HTTPS request')\n",
    "\n",
    "import requests\n",
    "_orig_request = requests.Session.request\n",
    "\n",
    "def _patched_request(self, method, url, **kwargs):\n",
    "    kwargs['verify'] = False\n",
    "    return _orig_request(self, method, url, **kwargs)\n",
    "\n",
    "requests.Session.request = _patched_request\n",
    "\n",
    "print(\"✓ SSL verification disabled (corporate network compatibility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Set Random Seeds for Reproducibility\n",
    "# ============================================================\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # For deterministic behavior (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for Python hashing\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(CONFIG[\"random_seed\"])\n",
    "print(f\"✓ Random seed set to {CONFIG['random_seed']} for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/automatic_speech_recognition/venv_asr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Import Required Libraries\n",
    "# ============================================================\n",
    "\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hugging Face libraries\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Hardware Detection and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Hardware Information\n",
      "======================================================================\n",
      "Device: Apple Silicon (MPS)\n",
      "FP16 Training: Enabled\n",
      "\n",
      "Device info saved to: outputs/whisper_azerbaijani_20260111_153802_device_info.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Detect Available Hardware\n",
    "# ============================================================\n",
    "\n",
    "def detect_device() -> Dict[str, Any]:\n",
    "    \"\"\"Detect available hardware and return device info.\"\"\"\n",
    "    device_info = {\n",
    "        \"device\": \"cpu\",\n",
    "        \"device_name\": \"CPU\",\n",
    "        \"memory_gb\": None,\n",
    "        \"fp16_available\": False,\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device_info[\"device\"] = \"cuda\"\n",
    "        device_info[\"device_name\"] = torch.cuda.get_device_name(0)\n",
    "        device_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        device_info[\"fp16_available\"] = True\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_info[\"device\"] = \"mps\"\n",
    "        device_info[\"device_name\"] = \"Apple Silicon (MPS)\"\n",
    "        device_info[\"fp16_available\"] = True\n",
    "    \n",
    "    return device_info\n",
    "\n",
    "device_info = detect_device()\n",
    "\n",
    "# Update config based on hardware\n",
    "if not device_info[\"fp16_available\"]:\n",
    "    CONFIG[\"fp16\"] = False\n",
    "\n",
    "# Save device info\n",
    "device_info_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_device_info.json\"\n",
    "with open(device_info_path, 'w') as f:\n",
    "    json.dump(device_info, f, indent=2)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Hardware Information\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {device_info['device_name']}\")\n",
    "if device_info['memory_gb']:\n",
    "    print(f\"Memory: {device_info['memory_gb']:.2f} GB\")\n",
    "print(f\"FP16 Training: {'Enabled' if CONFIG['fp16'] else 'Disabled'}\")\n",
    "print(f\"\\nDevice info saved to: {device_info_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading dataset: LocalDoc/azerbaijani_asr\n",
      "Mode: Sample (500 samples)\n",
      "Loading with streaming mode...\n",
      "Taking 500 samples from stream...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples: 100%|██████████| 500/500 [01:03<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset loaded successfully\n",
      "Train samples: 500\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load Dataset\n",
    "# ============================================================\n",
    "\n",
    "def load_asr_dataset(config: Dict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Load ASR dataset with streaming support.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict with train split\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {config['dataset_name']}\")\n",
    "    print(f\"Mode: {'Sample ({} samples)'.format(config['sample_size']) if config['sample_mode'] else 'Full dataset'}\")\n",
    "    \n",
    "    # Check if local data exists\n",
    "    local_path = DATA_DIR / \"dataset_cache\"\n",
    "    \n",
    "    if local_path.exists() and not config['sample_mode']:\n",
    "        print(f\"Loading from local cache: {local_path}\")\n",
    "        from datasets import load_from_disk\n",
    "        dataset = load_from_disk(str(local_path))\n",
    "    else:\n",
    "        print(\"Loading with streaming mode...\")\n",
    "        dataset_stream = load_dataset(\n",
    "            config['dataset_name'],\n",
    "            streaming=True,\n",
    "            trust_remote_code=False\n",
    "        )\n",
    "        \n",
    "        # Take samples if in sample mode\n",
    "        if config['sample_mode']:\n",
    "            n_samples = config['sample_size']\n",
    "            print(f\"Taking {n_samples} samples from stream...\")\n",
    "            \n",
    "            samples = list(tqdm(\n",
    "                dataset_stream[\"train\"].take(n_samples),\n",
    "                total=n_samples,\n",
    "                desc=\"Loading samples\"\n",
    "            ))\n",
    "            \n",
    "            dataset = DatasetDict({\n",
    "                \"train\": Dataset.from_list(samples)\n",
    "            })\n",
    "        else:\n",
    "            # For full dataset, download and cache\n",
    "            dataset = load_dataset(config['dataset_name'])\n",
    "            dataset.save_to_disk(str(local_path))\n",
    "            print(f\"Dataset cached to: {local_path}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load dataset\n",
    "print(\"=\" * 70)\n",
    "dataset = load_asr_dataset(CONFIG)\n",
    "print(\"\\n✓ Dataset loaded successfully\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Data Validation\n",
      "======================================================================\n",
      "\n",
      "Total Samples: 500\n",
      "Columns: audio, text, duration, audio_file\n",
      "Audio Column: audio\n",
      "Text Column: text\n",
      "\n",
      "Missing Values:\n",
      "  audio: 0\n",
      "  text: 0\n",
      "\n",
      "Duration Statistics:\n",
      "  mean: 5.75\n",
      "  median: 5.08\n",
      "  std: 2.95\n",
      "  min: 0.71\n",
      "  max: 16.57\n",
      "  total_hours: 0.80\n",
      "\n",
      "Text Statistics:\n",
      "  mean_length: 85.00\n",
      "  median_length: 77.50\n",
      "  min_length: 11.00\n",
      "  max_length: 237.00\n",
      "\n",
      "✓ No validation issues found\n",
      "\n",
      "Validation results saved to: outputs/whisper_azerbaijani_20260111_153802_validation.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Data Validation and Schema Checks\n",
    "# ============================================================\n",
    "\n",
    "def validate_dataset(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate dataset schema and content.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        \"total_samples\": len(dataset),\n",
    "        \"columns\": list(dataset.features.keys()),\n",
    "        \"missing_values\": {},\n",
    "        \"duration_stats\": {},\n",
    "        \"text_stats\": {},\n",
    "        \"issues\": [],\n",
    "    }\n",
    "    \n",
    "    # Check for required columns\n",
    "    sample = dataset[0]\n",
    "    audio_col = \"audio\" if \"audio\" in sample else \"path\"\n",
    "    text_col = next((col for col in [\"sentence\", \"text\", \"transcription\"] if col in sample), None)\n",
    "    \n",
    "    if audio_col not in sample:\n",
    "        validation_results[\"issues\"].append(\"Missing audio column\")\n",
    "    if text_col is None:\n",
    "        validation_results[\"issues\"].append(\"Missing text column\")\n",
    "    \n",
    "    validation_results[\"audio_column\"] = audio_col\n",
    "    validation_results[\"text_column\"] = text_col\n",
    "    \n",
    "    # Check for missing/empty values\n",
    "    for col in [audio_col, text_col]:\n",
    "        if col:\n",
    "            empty_count = sum(1 for item in dataset if not item.get(col))\n",
    "            validation_results[\"missing_values\"][col] = empty_count\n",
    "            if empty_count > 0:\n",
    "                validation_results[\"issues\"].append(f\"{col} has {empty_count} missing values\")\n",
    "    \n",
    "    # Analyze durations if available\n",
    "    if \"duration\" in sample:\n",
    "        durations = [item[\"duration\"] for item in dataset if item.get(\"duration\")]\n",
    "        validation_results[\"duration_stats\"] = {\n",
    "            \"mean\": np.mean(durations),\n",
    "            \"median\": np.median(durations),\n",
    "            \"std\": np.std(durations),\n",
    "            \"min\": np.min(durations),\n",
    "            \"max\": np.max(durations),\n",
    "            \"total_hours\": np.sum(durations) / 3600,\n",
    "        }\n",
    "    \n",
    "    # Analyze text lengths\n",
    "    if text_col:\n",
    "        text_lengths = [len(item[text_col]) for item in dataset if item.get(text_col)]\n",
    "        validation_results[\"text_stats\"] = {\n",
    "            \"mean_length\": np.mean(text_lengths),\n",
    "            \"median_length\": np.median(text_lengths),\n",
    "            \"min_length\": np.min(text_lengths),\n",
    "            \"max_length\": np.max(text_lengths),\n",
    "        }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"Data Validation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_results = validate_dataset(dataset[\"train\"])\n",
    "\n",
    "# Print validation results\n",
    "print(f\"\\nTotal Samples: {validation_results['total_samples']}\")\n",
    "print(f\"Columns: {', '.join(validation_results['columns'])}\")\n",
    "print(f\"Audio Column: {validation_results['audio_column']}\")\n",
    "print(f\"Text Column: {validation_results['text_column']}\")\n",
    "\n",
    "if validation_results['missing_values']:\n",
    "    print(\"\\nMissing Values:\")\n",
    "    for col, count in validation_results['missing_values'].items():\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "if validation_results['duration_stats']:\n",
    "    print(\"\\nDuration Statistics:\")\n",
    "    for key, value in validation_results['duration_stats'].items():\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "if validation_results['text_stats']:\n",
    "    print(\"\\nText Statistics:\")\n",
    "    for key, value in validation_results['text_stats'].items():\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "if validation_results['issues']:\n",
    "    print(\"\\n⚠️  Issues Found:\")\n",
    "    for issue in validation_results['issues']:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\n✓ No validation issues found\")\n",
    "\n",
    "# Save validation results\n",
    "validation_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_validation.json\"\n",
    "with open(validation_path, 'w') as f:\n",
    "    json.dump(validation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nValidation results saved to: {validation_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating exploratory data analysis visualizations...\n",
      "✓ Duration distribution chart saved\n",
      "✓ Text length distribution chart saved\n",
      "✓ Data summary table saved\n",
      "\n",
      "All visualizations saved to: charts\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Exploratory Data Analysis - Visualizations\n",
    "# ============================================================\n",
    "\n",
    "def create_eda_visualizations(dataset: Dataset, validation_results: Dict, save_dir: Path):\n",
    "    \"\"\"\n",
    "    Create exploratory data analysis visualizations.\n",
    "    \"\"\"\n",
    "    text_col = validation_results[\"text_column\"]\n",
    "    \n",
    "    # Duration distribution (if available)\n",
    "    if \"duration\" in dataset[0]:\n",
    "        durations = [item[\"duration\"] for item in dataset if item.get(\"duration\")]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        ax1.hist(durations, bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax1.set_xlabel('Duration (seconds)')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.set_title('Audio Duration Distribution')\n",
    "        ax1.axvline(np.mean(durations), color='r', linestyle='--', label=f'Mean: {np.mean(durations):.2f}s')\n",
    "        ax1.axvline(np.median(durations), color='g', linestyle='--', label=f'Median: {np.median(durations):.2f}s')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Box plot\n",
    "        ax2.boxplot(durations, vert=True)\n",
    "        ax2.set_ylabel('Duration (seconds)')\n",
    "        ax2.set_title('Audio Duration Box Plot')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"duration_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Duration distribution chart saved\")\n",
    "    \n",
    "    # Text length distribution\n",
    "    if text_col:\n",
    "        text_lengths = [len(item[text_col]) for item in dataset if item.get(text_col)]\n",
    "        word_counts = [len(item[text_col].split()) for item in dataset if item.get(text_col)]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Character length\n",
    "        ax1.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "        ax1.set_xlabel('Text Length (characters)')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.set_title('Transcription Length Distribution (Characters)')\n",
    "        ax1.axvline(np.mean(text_lengths), color='r', linestyle='--', label=f'Mean: {np.mean(text_lengths):.1f}')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Word count\n",
    "        ax2.hist(word_counts, bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "        ax2.set_xlabel('Word Count')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.set_title('Transcription Length Distribution (Words)')\n",
    "        ax2.axvline(np.mean(word_counts), color='r', linestyle='--', label=f'Mean: {np.mean(word_counts):.1f}')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"text_length_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Text length distribution chart saved\")\n",
    "    \n",
    "    # Summary statistics table\n",
    "    summary_data = []\n",
    "    if \"duration\" in dataset[0]:\n",
    "        durations = [item[\"duration\"] for item in dataset if item.get(\"duration\")]\n",
    "        summary_data.append(['Duration (sec)', f\"{np.mean(durations):.2f}\", f\"{np.median(durations):.2f}\", \n",
    "                            f\"{np.std(durations):.2f}\", f\"{np.min(durations):.2f}\", f\"{np.max(durations):.2f}\"])\n",
    "    \n",
    "    if text_col:\n",
    "        text_lengths = [len(item[text_col]) for item in dataset if item.get(text_col)]\n",
    "        summary_data.append(['Text Length (chars)', f\"{np.mean(text_lengths):.1f}\", f\"{np.median(text_lengths):.1f}\",\n",
    "                            f\"{np.std(text_lengths):.1f}\", f\"{np.min(text_lengths):.0f}\", f\"{np.max(text_lengths):.0f}\"])\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data, columns=['Metric', 'Mean', 'Median', 'Std', 'Min', 'Max'])\n",
    "        summary_df.to_csv(OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_data_summary.csv\", index=False)\n",
    "        print(\"✓ Data summary table saved\")\n",
    "\n",
    "# Create visualizations\n",
    "print(\"\\nCreating exploratory data analysis visualizations...\")\n",
    "create_eda_visualizations(dataset[\"train\"], validation_results, CHARTS_DIR)\n",
    "print(f\"\\nAll visualizations saved to: {CHARTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Splitting (No Data Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating Data Splits\n",
      "======================================================================\n",
      "\n",
      "Split Sizes:\n",
      "  Train:         400 (80%)\n",
      "  Validation:     50 (10%)\n",
      "  Test:           50 (10%)\n",
      "  Total:         500\n",
      "\n",
      "✓ Split information saved to: outputs/whisper_azerbaijani_20260111_153802_split_info.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create Train/Val/Test Splits\n",
    "# ============================================================\n",
    "\n",
    "def create_splits(dataset: Dataset, config: Dict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits with fixed random seed.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Input dataset\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict with train, validation, and test splits\n",
    "    \"\"\"\n",
    "    # Reset seed for reproducibility\n",
    "    set_seed(config[\"random_seed\"])\n",
    "    \n",
    "    train_ratio = config[\"train_ratio\"]\n",
    "    val_ratio = config[\"val_ratio\"]\n",
    "    test_ratio = config[\"test_ratio\"]\n",
    "    \n",
    "    # Validate ratios\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
    "    \n",
    "    # First split: train and temp (val + test)\n",
    "    train_test_split = dataset.train_test_split(\n",
    "        test_size=(val_ratio + test_ratio),\n",
    "        seed=config[\"random_seed\"]\n",
    "    )\n",
    "    \n",
    "    # Second split: val and test from temp\n",
    "    val_test_ratio = test_ratio / (val_ratio + test_ratio)\n",
    "    val_test_split = train_test_split[\"test\"].train_test_split(\n",
    "        test_size=val_test_ratio,\n",
    "        seed=config[\"random_seed\"]\n",
    "    )\n",
    "    \n",
    "    splits = DatasetDict({\n",
    "        \"train\": train_test_split[\"train\"],\n",
    "        \"validation\": val_test_split[\"train\"],\n",
    "        \"test\": val_test_split[\"test\"],\n",
    "    })\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Create splits\n",
    "print(\"=\" * 70)\n",
    "print(\"Creating Data Splits\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_splits = create_splits(dataset[\"train\"], CONFIG)\n",
    "\n",
    "split_info = {\n",
    "    \"train\": len(dataset_splits[\"train\"]),\n",
    "    \"validation\": len(dataset_splits[\"validation\"]),\n",
    "    \"test\": len(dataset_splits[\"test\"]),\n",
    "    \"train_ratio\": CONFIG[\"train_ratio\"],\n",
    "    \"val_ratio\": CONFIG[\"val_ratio\"],\n",
    "    \"test_ratio\": CONFIG[\"test_ratio\"],\n",
    "    \"random_seed\": CONFIG[\"random_seed\"],\n",
    "}\n",
    "\n",
    "print(f\"\\nSplit Sizes:\")\n",
    "print(f\"  Train:      {split_info['train']:>6} ({CONFIG['train_ratio']*100:.0f}%)\")\n",
    "print(f\"  Validation: {split_info['validation']:>6} ({CONFIG['val_ratio']*100:.0f}%)\")\n",
    "print(f\"  Test:       {split_info['test']:>6} ({CONFIG['test_ratio']*100:.0f}%)\")\n",
    "print(f\"  Total:      {sum([split_info['train'], split_info['validation'], split_info['test']]):>6}\")\n",
    "\n",
    "# Save split info\n",
    "split_info_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_split_info.json\"\n",
    "with open(split_info_path, 'w') as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Split information saved to: {split_info_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading Model: openai/whisper-tiny\n",
      "======================================================================\n",
      "\n",
      "✓ Model loaded successfully\n",
      "Parameters: 37,760,640\n",
      "Vocabulary Size: 51,865\n",
      "Model Dimension: 384\n",
      "\n",
      "Model information saved to: outputs/whisper_azerbaijani_20260111_153802_model_info.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load Whisper Model and Processor\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Loading Model: {CONFIG['model_name']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load processor (combines tokenizer and feature extractor)\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    language=CONFIG[\"language\"],\n",
    "    task=CONFIG[\"task\"]\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# Configure model for Azerbaijani\n",
    "model.generation_config.language = CONFIG[\"language\"]\n",
    "model.generation_config.task = CONFIG[\"task\"]\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Model information\n",
    "model_info = {\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"num_parameters\": model.num_parameters(),\n",
    "    \"language\": CONFIG[\"language\"],\n",
    "    \"task\": CONFIG[\"task\"],\n",
    "    \"vocab_size\": model.config.vocab_size,\n",
    "    \"d_model\": model.config.d_model,\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully\")\n",
    "print(f\"Parameters: {model_info['num_parameters']:,}\")\n",
    "print(f\"Vocabulary Size: {model_info['vocab_size']:,}\")\n",
    "print(f\"Model Dimension: {model_info['d_model']}\")\n",
    "\n",
    "# Save model info\n",
    "model_info_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_model_info.json\"\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel information saved to: {model_info_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Preprocessing Dataset\n",
      "======================================================================\n",
      "Audio column: audio\n",
      "Text column: text\n",
      "Target sampling rate: 16000 Hz\n",
      "\n",
      "Processing splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 400/400 [00:03<00:00, 110.27 examples/s]\n",
      "Preprocessing: 100%|██████████| 50/50 [00:00<00:00, 128.82 examples/s]\n",
      "Preprocessing: 100%|██████████| 50/50 [00:00<00:00, 138.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All splits preprocessed\n",
      "  Train: 400 samples\n",
      "  Validation: 50 samples\n",
      "  Test: 50 samples\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Prepare Dataset for Training\n",
    "# ============================================================\n",
    "\n",
    "# Get column names from validation results\n",
    "audio_column = validation_results[\"audio_column\"]\n",
    "text_column = validation_results[\"text_column\"]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Preprocessing Dataset\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Audio column: {audio_column}\")\n",
    "print(f\"Text column: {text_column}\")\n",
    "print(f\"Target sampling rate: {CONFIG['sampling_rate']} Hz\")\n",
    "\n",
    "# Cast audio to correct sampling rate\n",
    "dataset_splits = dataset_splits.cast_column(\n",
    "    audio_column,\n",
    "    Audio(sampling_rate=CONFIG[\"sampling_rate\"])\n",
    ")\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Prepare a batch for training.\n",
    "    Converts audio to features and tokenizes text.\n",
    "    \"\"\"\n",
    "    # Extract audio\n",
    "    audio = batch[audio_column]\n",
    "    \n",
    "    # Compute input features\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Tokenize text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[text_column]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Process all splits\n",
    "print(\"\\nProcessing splits...\")\n",
    "processed_datasets = dataset_splits.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset_splits[\"train\"].column_names,\n",
    "    desc=\"Preprocessing\",\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ All splits preprocessed\")\n",
    "print(f\"  Train: {len(processed_datasets['train'])} samples\")\n",
    "print(f\"  Validation: {len(processed_datasets['validation'])} samples\")\n",
    "print(f\"  Test: {len(processed_datasets['test'])} samples\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data collator created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Data Collator for Dynamic Padding\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs and labels.\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        features: List[Dict[str, torch.Tensor]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Replace padding with -100 to ignore in loss computation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1),\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "print(\"✓ Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.13kB [00:00, 4.84MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation metrics configured\n",
      "  Metric: Word Error Rate (WER)\n",
      "  Lower is better (0% = perfect transcription)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Define Evaluation Metrics\n",
    "# ============================================================\n",
    "\n",
    "# Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute Word Error Rate (WER) during evaluation.\n",
    "    \n",
    "    Lower WER is better (0% is perfect).\n",
    "    \"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad token\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"✓ Evaluation metrics configured\")\n",
    "print(\"  Metric: Word Error Rate (WER)\")\n",
    "print(\"  Lower is better (0% = perfect transcription)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training Configuration\n",
      "======================================================================\n",
      "Output directory: artifacts/whisper_azerbaijani_20260111_153802\n",
      "\n",
      "Hyperparameters:\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 2\n",
      "  Effective batch size: 8\n",
      "  Learning rate: 1e-05\n",
      "  Epochs: 1\n",
      "  Max steps: 100\n",
      "  Warmup steps: 20\n",
      "  FP16: True\n",
      "\n",
      "Evaluation:\n",
      "  Eval every: 50 steps\n",
      "  Save every: 50 steps\n",
      "  Keep best: 3 checkpoints\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Training Arguments\n",
    "# ============================================================\n",
    "\n",
    "# Create output directory for this experiment\n",
    "experiment_artifacts_dir = ARTIFACTS_DIR / CONFIG[\"experiment_name\"]\n",
    "experiment_artifacts_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(experiment_artifacts_dir),\n",
    "    \n",
    "    # Training parameters\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # Precision\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Generation settings for evaluation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    logging_dir=str(experiment_artifacts_dir / \"logs\"),\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Device\n",
    "    use_cpu=(device_info[\"device\"] == \"cpu\"),\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=CONFIG[\"random_seed\"],\n",
    "    data_seed=CONFIG[\"random_seed\"],\n",
    "    \n",
    "    # Misc\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Output directory: {experiment_artifacts_dir}\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Max steps: {CONFIG['max_steps'] if CONFIG['max_steps'] > 0 else 'Full epochs'}\")\n",
    "print(f\"  Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "print(f\"  FP16: {CONFIG['fp16']}\")\n",
    "print(f\"\\nEvaluation:\")\n",
    "print(f\"  Eval every: {CONFIG['eval_steps']} steps\")\n",
    "print(f\"  Save every: {CONFIG['save_steps']} steps\")\n",
    "print(f\"  Keep best: {CONFIG['save_total_limit']} checkpoints\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training metrics callback created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Custom Callback for Tracking Training Progress\n",
    "# ============================================================\n",
    "\n",
    "class TrainingMetricsCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to track and save training metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.training_history = []\n",
    "        self.eval_history = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            logs_copy = logs.copy()\n",
    "            logs_copy[\"step\"] = state.global_step\n",
    "            logs_copy[\"epoch\"] = state.epoch\n",
    "            \n",
    "            if \"loss\" in logs:\n",
    "                self.training_history.append(logs_copy)\n",
    "            if \"eval_wer\" in logs:\n",
    "                self.eval_history.append(logs_copy)\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # Save training history\n",
    "        if self.training_history:\n",
    "            train_df = pd.DataFrame(self.training_history)\n",
    "            train_df.to_csv(\n",
    "                self.output_dir / f\"{CONFIG['experiment_name']}_training_history.csv\",\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # Save evaluation history\n",
    "        if self.eval_history:\n",
    "            eval_df = pd.DataFrame(self.eval_history)\n",
    "            eval_df.to_csv(\n",
    "                self.output_dir / f\"{CONFIG['experiment_name']}_eval_history.csv\",\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "# Create callback\n",
    "metrics_callback = TrainingMetricsCallback(OUTPUTS_DIR)\n",
    "\n",
    "print(\"✓ Training metrics callback created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer initialized\n",
      "  Train samples: 400\n",
      "  Validation samples: 50\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Initialize Trainer\n",
    "# ============================================================\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor,\n",
    "    callbacks=[metrics_callback],\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"  Train samples: {len(processed_datasets['train'])}\")\n",
    "print(f\"  Validation samples: {len(processed_datasets['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting Training\n",
      "======================================================================\n",
      "Device: Apple Silicon (MPS)\n",
      "Mode: SAMPLE\n",
      "\n",
      "This may take a while...\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/automatic_speech_recognition/venv_asr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Train Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {device_info['device_name']}\")\n",
    "print(f\"Mode: {'SAMPLE' if CONFIG['sample_mode'] else 'FULL'}\")\n",
    "print(\"\\nThis may take a while...\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training Completed!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save training metrics\n",
    "training_metrics = {\n",
    "    \"train_runtime\": train_result.metrics.get(\"train_runtime\", 0),\n",
    "    \"train_samples_per_second\": train_result.metrics.get(\"train_samples_per_second\", 0),\n",
    "    \"train_steps_per_second\": train_result.metrics.get(\"train_steps_per_second\", 0),\n",
    "    \"total_flos\": train_result.metrics.get(\"total_flos\", 0),\n",
    "    \"train_loss\": train_result.metrics.get(\"train_loss\", 0),\n",
    "    \"epoch\": train_result.metrics.get(\"epoch\", 0),\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Runtime: {training_metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  Samples/second: {training_metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"  Final loss: {training_metrics['train_loss']:.4f}\")\n",
    "print(f\"  Epochs completed: {training_metrics['epoch']:.2f}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_training_metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining metrics saved to: {metrics_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluate on Validation Set\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Evaluating on Validation Set\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "val_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "for key, value in val_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save results\n",
    "val_results_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_validation_results.json\"\n",
    "with open(val_results_path, 'w') as f:\n",
    "    json.dump(val_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nValidation results saved to: {val_results_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluate on Test Set (Final Evaluation)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Evaluating on Test Set (Hold-out)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save results\n",
    "test_results_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_test_results.json\"\n",
    "with open(test_results_path, 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nTest results saved to: {test_results_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Sample Predictions\n",
    "# ============================================================\n",
    "\n",
    "def generate_sample_predictions(\n",
    "    trainer: Seq2SeqTrainer,\n",
    "    dataset: Dataset,\n",
    "    processor: WhisperProcessor,\n",
    "    n_samples: int = 10\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate predictions for sample test cases.\n",
    "    \"\"\"\n",
    "    # Get random samples\n",
    "    indices = random.sample(range(len(dataset)), min(n_samples, len(dataset)))\n",
    "    samples = dataset.select(indices)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = trainer.predict(samples)\n",
    "    pred_ids = predictions.predictions\n",
    "    label_ids = predictions.label_ids\n",
    "    \n",
    "    # Decode\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Create results\n",
    "    results = []\n",
    "    for i, (pred, label) in enumerate(zip(pred_str, label_str)):\n",
    "        results.append({\n",
    "            \"sample_id\": i + 1,\n",
    "            \"reference\": label,\n",
    "            \"prediction\": pred,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Generating sample predictions...\")\n",
    "sample_predictions = generate_sample_predictions(\n",
    "    trainer,\n",
    "    processed_datasets[\"test\"],\n",
    "    processor,\n",
    "    n_samples=10\n",
    ")\n",
    "\n",
    "# Display samples\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Sample Predictions\")\n",
    "print(\"=\" * 70)\n",
    "for sample in sample_predictions[:5]:  # Show first 5\n",
    "    print(f\"\\nSample {sample['sample_id']}:\")\n",
    "    print(f\"  Reference:  {sample['reference']}\")\n",
    "    print(f\"  Prediction: {sample['prediction']}\")\n",
    "\n",
    "# Save all predictions\n",
    "predictions_df = pd.DataFrame(sample_predictions)\n",
    "predictions_path = OUTPUTS_DIR / f\"{CONFIG['experiment_name']}_sample_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "print(f\"\\nAll sample predictions saved to: {predictions_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Training Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create Training Visualizations\n",
    "# ============================================================\n",
    "\n",
    "def plot_training_curves(metrics_callback: TrainingMetricsCallback, save_dir: Path):\n",
    "    \"\"\"\n",
    "    Create comprehensive training visualization plots.\n",
    "    \"\"\"\n",
    "    if not metrics_callback.training_history:\n",
    "        print(\"No training history available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Convert to dataframes\n",
    "    train_df = pd.DataFrame(metrics_callback.training_history)\n",
    "    eval_df = pd.DataFrame(metrics_callback.eval_history) if metrics_callback.eval_history else None\n",
    "    \n",
    "    # Training loss curve\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(train_df['step'], train_df['loss'], label='Training Loss', linewidth=2)\n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss Over Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / \"training_loss_curve.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Training loss curve saved\")\n",
    "    \n",
    "    # Evaluation WER curve\n",
    "    if eval_df is not None and not eval_df.empty:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(eval_df['step'], eval_df['eval_wer'], label='Validation WER', \n",
    "                color='orange', linewidth=2, marker='o')\n",
    "        ax.set_xlabel('Training Steps')\n",
    "        ax.set_ylabel('Word Error Rate (%)')\n",
    "        ax.set_title('Validation WER Over Time (Lower is Better)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"validation_wer_curve.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Validation WER curve saved\")\n",
    "    \n",
    "    # Combined plot\n",
    "    if eval_df is not None and not eval_df.empty:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(train_df['step'], train_df['loss'], linewidth=2)\n",
    "        ax1.set_xlabel('Training Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # WER\n",
    "        ax2.plot(eval_df['step'], eval_df['eval_wer'], color='orange', \n",
    "                linewidth=2, marker='o', markersize=4)\n",
    "        ax2.set_xlabel('Training Steps')\n",
    "        ax2.set_ylabel('Word Error Rate (%)')\n",
    "        ax2.set_title('Validation WER')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"training_overview.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Training overview saved\")\n",
    "\n",
    "# Create plots\n",
    "print(\"\\nCreating training visualizations...\")\n",
    "plot_training_curves(metrics_callback, CHARTS_DIR)\n",
    "print(f\"\\nAll charts saved to: {CHARTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create Final Results Summary Visualization\n",
    "# ============================================================\n",
    "\n",
    "def create_results_summary(train_metrics, val_results, test_results, save_dir: Path):\n",
    "    \"\"\"\n",
    "    Create a comprehensive results summary visualization.\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. WER Comparison\n",
    "    wer_data = {\n",
    "        'Validation': val_results.get('eval_wer', 0),\n",
    "        'Test': test_results.get('eval_wer', 0),\n",
    "    }\n",
    "    ax1.bar(wer_data.keys(), wer_data.values(), color=['#2ecc71', '#e74c3c'])\n",
    "    ax1.set_ylabel('Word Error Rate (%)')\n",
    "    ax1.set_title('Model Performance (Lower is Better)')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    for i, (k, v) in enumerate(wer_data.items()):\n",
    "        ax1.text(i, v + 0.5, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Training Time Breakdown\n",
    "    runtime_hours = train_metrics['train_runtime'] / 3600\n",
    "    ax2.bar(['Training Time'], [runtime_hours], color='#3498db')\n",
    "    ax2.set_ylabel('Hours')\n",
    "    ax2.set_title('Training Duration')\n",
    "    ax2.text(0, runtime_hours + runtime_hours*0.05, f'{runtime_hours:.2f}h', \n",
    "            ha='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Dataset Split Sizes\n",
    "    split_sizes = {\n",
    "        'Train': len(processed_datasets['train']),\n",
    "        'Validation': len(processed_datasets['validation']),\n",
    "        'Test': len(processed_datasets['test']),\n",
    "    }\n",
    "    ax3.bar(split_sizes.keys(), split_sizes.values(), color=['#9b59b6', '#f39c12', '#1abc9c'])\n",
    "    ax3.set_ylabel('Number of Samples')\n",
    "    ax3.set_title('Dataset Split Sizes')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Training Metrics Summary\n",
    "    metrics_text = f\"\"\"\n",
    "    Experiment: {CONFIG['experiment_name']}\n",
    "    \n",
    "    Model: {CONFIG['model_name']}\n",
    "    Parameters: {model_info['num_parameters']:,}\n",
    "    \n",
    "    Training:\n",
    "    - Epochs: {train_metrics['epoch']:.2f}\n",
    "    - Final Loss: {train_metrics['train_loss']:.4f}\n",
    "    - Samples/sec: {train_metrics['train_samples_per_second']:.2f}\n",
    "    \n",
    "    Validation WER: {val_results.get('eval_wer', 0):.2f}%\n",
    "    Test WER: {test_results.get('eval_wer', 0):.2f}%\n",
    "    \n",
    "    Device: {device_info['device_name']}\n",
    "    FP16: {CONFIG['fp16']}\n",
    "    Batch Size: {CONFIG['batch_size']}\n",
    "    Learning Rate: {CONFIG['learning_rate']}\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, metrics_text, fontsize=10, verticalalignment='center',\n",
    "            fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Training Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / \"results_summary.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Results summary visualization saved\")\n",
    "\n",
    "# Create summary\n",
    "print(\"\\nCreating results summary...\")\n",
    "create_results_summary(training_metrics, val_results, test_results, CHARTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save Final Model and Artifacts\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Saving Model Artifacts\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create final model directory\n",
    "final_model_dir = ARTIFACTS_DIR / f\"{CONFIG['experiment_name']}_final\"\n",
    "final_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save model and processor\n",
    "trainer.save_model(str(final_model_dir))\n",
    "processor.save_pretrained(str(final_model_dir))\n",
    "\n",
    "print(f\"\\n✓ Model saved to: {final_model_dir}\")\n",
    "\n",
    "# Save complete experiment metadata\n",
    "experiment_metadata = {\n",
    "    \"experiment_name\": CONFIG[\"experiment_name\"],\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": CONFIG,\n",
    "    \"device_info\": device_info,\n",
    "    \"model_info\": model_info,\n",
    "    \"split_info\": split_info,\n",
    "    \"training_metrics\": training_metrics,\n",
    "    \"validation_results\": val_results,\n",
    "    \"test_results\": test_results,\n",
    "    \"model_path\": str(final_model_dir),\n",
    "}\n",
    "\n",
    "metadata_path = final_model_dir / \"experiment_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(experiment_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Experiment metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Create README for the model\n",
    "readme_content = f\"\"\"# Azerbaijani ASR Model\n",
    "\n",
    "## Experiment: {CONFIG['experiment_name']}\n",
    "\n",
    "### Model Information\n",
    "- Base Model: {CONFIG['model_name']}\n",
    "- Language: {CONFIG['language']}\n",
    "- Task: {CONFIG['task']}\n",
    "- Parameters: {model_info['num_parameters']:,}\n",
    "\n",
    "### Performance\n",
    "- Validation WER: {val_results.get('eval_wer', 0):.2f}%\n",
    "- Test WER: {test_results.get('eval_wer', 0):.2f}%\n",
    "\n",
    "### Training Details\n",
    "- Training Samples: {split_info['train']}\n",
    "- Validation Samples: {split_info['validation']}\n",
    "- Test Samples: {split_info['test']}\n",
    "- Epochs: {training_metrics['epoch']:.2f}\n",
    "- Training Time: {training_metrics['train_runtime']/3600:.2f} hours\n",
    "- Device: {device_info['device_name']}\n",
    "\n",
    "### Usage\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"{final_model_dir}\"\n",
    ")\n",
    "\n",
    "# Transcribe audio\n",
    "result = pipe(\"path/to/audio.wav\")\n",
    "print(result[\"text\"])\n",
    "```\n",
    "\n",
    "### Files\n",
    "- `config.json` - Model configuration\n",
    "- `preprocessor_config.json` - Audio preprocessing config\n",
    "- `tokenizer_config.json` - Tokenizer configuration\n",
    "- `model.safetensors` - Model weights\n",
    "- `experiment_metadata.json` - Complete experiment details\n",
    "\n",
    "### Reproducibility\n",
    "- Random Seed: {CONFIG['random_seed']}\n",
    "- All configuration and results saved in experiment_metadata.json\n",
    "\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "readme_path = final_model_dir / \"README.md\"\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"✓ Model README saved to: {readme_path}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All artifacts saved successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Inference Pipeline\n",
    "# ============================================================\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Testing Inference Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load inference pipeline\n",
    "device_id = 0 if device_info[\"device\"] == \"cuda\" else -1\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=str(final_model_dir),\n",
    "    device=device_id,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Inference pipeline loaded successfully\")\n",
    "print(f\"  Model: {final_model_dir}\")\n",
    "print(f\"  Device: {device_info['device']}\")\n",
    "\n",
    "print(\"\\nTo use the model for inference:\")\n",
    "print(\"\"\"\\n```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"{}\"\n",
    ")\n",
    "\n",
    "# Transcribe audio file\n",
    "result = pipe(\"path/to/audio.wav\")\n",
    "print(result[\"text\"])\n",
    "\n",
    "# Or with audio array\n",
    "import librosa\n",
    "audio, sr = librosa.load(\"path/to/audio.wav\", sr=16000)\n",
    "result = pipe(audio)\n",
    "print(result[\"text\"])\n",
    "```\"\"\".format(final_model_dir))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Final Report\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nExperiment: {CONFIG['experiment_name']}\")\n",
    "print(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Validation WER: {val_results.get('eval_wer', 0):.2f}%\")\n",
    "print(f\"Test WER: {test_results.get('eval_wer', 0):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"DATASET\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Train: {split_info['train']} samples\")\n",
    "print(f\"Validation: {split_info['validation']} samples\")\n",
    "print(f\"Test: {split_info['test']} samples\")\n",
    "print(f\"Total: {sum([split_info['train'], split_info['validation'], split_info['test']])} samples\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"TRAINING\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Runtime: {training_metrics['train_runtime']/3600:.2f} hours\")\n",
    "print(f\"Epochs: {training_metrics['epoch']:.2f}\")\n",
    "print(f\"Final Loss: {training_metrics['train_loss']:.4f}\")\n",
    "print(f\"Throughput: {training_metrics['train_samples_per_second']:.2f} samples/sec\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ARTIFACTS LOCATIONS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Model: {final_model_dir}\")\n",
    "print(f\"Charts: {CHARTS_DIR}\")\n",
    "print(f\"Outputs: {OUTPUTS_DIR}\")\n",
    "print(f\"All Artifacts: {ARTIFACTS_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# List all generated files\n",
    "print(\"\\nCharts:\")\n",
    "for f in sorted(CHARTS_DIR.glob(\"*.png\")):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nOutputs:\")\n",
    "for f in sorted(OUTPUTS_DIR.glob(f\"{CONFIG['experiment_name']}*\")):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nModel Files:\")\n",
    "for f in sorted(final_model_dir.glob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n✓ All stages completed\")\n",
    "print(\"✓ Model trained and evaluated\")\n",
    "print(\"✓ All artifacts saved\")\n",
    "print(\"✓ Visualizations generated\")\n",
    "print(\"✓ Results documented\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review visualizations in the charts/ directory\")\n",
    "print(\"  2. Examine detailed metrics in the outputs/ directory\")\n",
    "print(\"  3. Load the model from artifacts/ for inference\")\n",
    "print(\"  4. For full training, set SAMPLE_MODE=False and rerun\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
